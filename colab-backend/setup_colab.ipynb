{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DepthAnything V2 æ·±åº¦æ¨å®šAPI - Google Colabè¨­å®š\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ã£ã¦ã€Google Colabä¸Šã§æ·±åº¦æ¨å®šAPIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPUä½¿ç”¨å¯èƒ½ã‹ç¢ºèª\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision transformers\n",
        "!pip install gradio\n",
        "!pip install pyngrok\n",
        "!pip install opencv-python-headless\n",
        "!pip install Pillow numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ngrokãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š\n",
        "\n",
        "1. [ngrok.com](https://ngrok.com)ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ\n",
        "2. Dashboard > Your Authtoken ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚³ãƒ”ãƒ¼\n",
        "3. ä¸‹ã®ã‚»ãƒ«ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ngrokãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\n",
        "import pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ã“ã“ã«ngrokãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›\n",
        "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆ\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "print(\"ngrokèªè¨¼å®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. æ·±åº¦æ¨å®šAPIã‚³ãƒ¼ãƒ‰ã®æº–å‚™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# APIã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ\n",
        "api_code = '''\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoImageProcessor, AutoModelForDepthEstimation\n",
        "import cv2\n",
        "from pyngrok import ngrok\n",
        "\n",
        "class DepthEstimationAPI:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        # DepthAnything V2ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
        "        self.model_name = \"depth-anything/Depth-Anything-V2-Small-hf\"\n",
        "        print(f\"Loading model: {self.model_name}\")\n",
        "        self.processor = AutoImageProcessor.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForDepthEstimation.from_pretrained(self.model_name)\n",
        "        self.model.to(self.device)\n",
        "        \n",
        "        print(f\"Model loaded successfully on {self.device}\")\n",
        "    \n",
        "    def estimate_depth(self, image):\n",
        "        \"\"\"æ·±åº¦æ¨å®šã®ãƒ¡ã‚¤ãƒ³å‡¦ç†\"\"\"\n",
        "        try:\n",
        "            if image is None:\n",
        "                return None, None\n",
        "            \n",
        "            # RGBã«å¤‰æ›\n",
        "            if image.mode != \\'RGB\\':\n",
        "                image = image.convert(\\'RGB\\')\n",
        "            \n",
        "            original_size = image.size\n",
        "            \n",
        "            # ãƒ¢ãƒ‡ãƒ«æ¨è«–\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                predicted_depth = outputs.predicted_depth\n",
        "            \n",
        "            # æ·±åº¦ãƒãƒƒãƒ—ã®å¾Œå‡¦ç†\n",
        "            depth = predicted_depth.squeeze().cpu().numpy()\n",
        "            \n",
        "            # æ­£è¦åŒ– (0-255)\n",
        "            depth_min = depth.min()\n",
        "            depth_max = depth.max()\n",
        "            depth_normalized = ((depth - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)\n",
        "            \n",
        "            # å…ƒã®ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º\n",
        "            depth_resized = cv2.resize(depth_normalized, original_size, interpolation=cv2.INTER_LINEAR)\n",
        "            \n",
        "            # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’é©ç”¨ (viridis)\n",
        "            depth_colored = cv2.applyColorMap(depth_resized, cv2.COLORMAP_VIRIDIS)\n",
        "            depth_colored = cv2.cvtColor(depth_colored, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # PILã‚¤ãƒ¡ãƒ¼ã‚¸ã«å¤‰æ›\n",
        "            depth_image = Image.fromarray(depth_colored)\n",
        "            \n",
        "            return image, depth_image\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in depth estimation: {e}\")\n",
        "            return None, None\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«API ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
        "print(\"Initializing DepthEstimation API...\")\n",
        "api = DepthEstimationAPI()\n",
        "\n",
        "def gradio_interface():\n",
        "    \"\"\"Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®è¨­å®š\"\"\"\n",
        "    \n",
        "    interface = gr.Interface(\n",
        "        fn=api.estimate_depth,\n",
        "        inputs=gr.Image(type=\"pil\", label=\"å…¥åŠ›ç”»åƒ\"),\n",
        "        outputs=[\n",
        "            gr.Image(type=\"pil\", label=\"å…ƒç”»åƒ\"),\n",
        "            gr.Image(type=\"pil\", label=\"æ·±åº¦ãƒãƒƒãƒ—\")\n",
        "        ],\n",
        "        title=\"DepthAnything V2 æ·±åº¦æ¨å®šAPI\",\n",
        "        description=\"DepthAnything V2ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦æ·±åº¦æ¨å®š\",\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
        "print(\"Setting up Gradio interface...\")\n",
        "demo = gradio_interface()\n",
        "\n",
        "# ngrokã§ãƒ‘ãƒ–ãƒªãƒƒã‚¯URLã‚’å–å¾—\n",
        "print(\"Setting up ngrok tunnel...\")\n",
        "public_url = ngrok.connect(7860)\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"ğŸš€ API Server is running!\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "print(f\"\\nğŸ“‹ Frontendç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„:\")\n",
        "print(f\"NEXT_PUBLIC_BACKEND_URL={public_url}\")\n",
        "print(f\"=\" * 60)\n",
        "\n",
        "# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•\n",
        "demo.launch(\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860,\n",
        "    share=False,\n",
        "    debug=True\n",
        ")\n",
        "'''\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
        "with open('depth_api.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(api_code)\n",
        "\n",
        "print(\"APIã‚³ãƒ¼ãƒ‰æº–å‚™å®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•\n",
        "\n",
        "ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨APIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã€ãƒ‘ãƒ–ãƒªãƒƒã‚¯URLãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# APIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•\n",
        "exec(open('depth_api.py').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è¨­å®š\n",
        "\n",
        "ä¸Šã§è¡¨ç¤ºã•ã‚ŒãŸURLã‚’ã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "```bash\n",
        "# frontend/.env.local\n",
        "NEXT_PUBLIC_BACKEND_URL=https://xxxxxxxx.ngrok.io\n",
        "```\n",
        "\n",
        "## ä½¿ç”¨ä¸Šã®æ³¨æ„\n",
        "\n",
        "1. **Colab ã‚»ãƒƒã‚·ãƒ§ãƒ³**: Colabã¯12æ™‚é–“ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¾ã™\n",
        "2. **ngrok URL**: ç„¡æ–™ç‰ˆã¯8æ™‚é–“ã§æœŸé™åˆ‡ã‚Œã«ãªã‚Šã¾ã™\n",
        "3. **GPUä½¿ç”¨é‡**: T4 GPUã‚’åŠ¹ç‡çš„ã«ä½¿ç”¨ã—ã¾ã™\n",
        "4. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ç´„2-3GBç¨‹åº¦ä½¿ç”¨ã—ã¾ã™\n",
        "\n",
        "## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
        "\n",
        "- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n",
        "- ngrokæ¥ç¶šã‚¨ãƒ©ãƒ¼: ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„\n",
        "- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}