#!/usr/bin/env python3
"""
Render Free Tier Optimized Server
Memory-efficient depth estimation with model loading optimization
"""
import os
import sys
import gc
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Depth Estimation API (Free Tier)",
    description="Memory-optimized API for depth estimation",
    version="1.0.0"
)

# CORS middleware - Allow all origins for free tier
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# Global variables for lazy loading
depth_estimator = None
available_models = {
    "Intel/dpt-hybrid-midas": "MiDaS (ËªΩÈáè)",
    "Intel/dpt-large": "DPT-Large (È´òÁ≤æÂ∫¶„ÉªÈáç„ÅÑ)",
    "LiheYoung/depth-anything-large-hf": "DepthAnything (Ê±éÁî®„ÉªÈáç„ÅÑ)"
}

def get_memory_usage():
    """Get current memory usage in MB"""
    try:
        import psutil
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    except ImportError:
        return 0  # psutil not available

def load_depth_estimator():
    """Lazy load depth estimator only when needed"""
    global depth_estimator
    
    if depth_estimator is None:
        try:
            logger.info(f"üíæ Loading depth estimator (Memory: {get_memory_usage():.1f}MB)")
            
            # Import here to avoid loading on startup
            from transformers import DPTImageProcessor, DPTForDepthEstimation
            from PIL import Image
            import torch
            import numpy as np
            
            # Use the lightest model for free tier
            model_name = "Intel/dpt-hybrid-midas"  # Smaller than dpt-large
            
            # Load with CPU-only to save memory
            processor = DPTImageProcessor.from_pretrained(model_name)
            model = DPTForDepthEstimation.from_pretrained(
                model_name,
                torch_dtype=torch.float32,  # Use float32 for CPU
                low_cpu_mem_usage=True
            )
            
            depth_estimator = {
                'processor': processor,
                'model': model,
                'model_name': model_name
            }
            
            logger.info(f"‚úÖ Model loaded (Memory: {get_memory_usage():.1f}MB)")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load model: {e}")
            raise HTTPException(status_code=500, detail=f"Model loading failed: {str(e)}")
    
    return depth_estimator

@app.get("/")
async def root():
    memory_mb = get_memory_usage()
    return {
        "message": "Depth Estimation API (Free Tier)",
        "version": "1.0.0",
        "status": "running",
        "memory_usage_mb": round(memory_mb, 1),
        "port": os.getenv("PORT", "not set"),
        "environment": "production"
    }

@app.get("/health")
async def health_check():
    memory_mb = get_memory_usage()
    return {
        "status": "healthy",
        "memory_usage_mb": round(memory_mb, 1),
        "port": os.getenv("PORT", "not set"),
        "host": "0.0.0.0"
    }

@app.get("/cors-test")
async def cors_test():
    """CORS test endpoint"""
    return {
        "message": "CORS test successful",
        "timestamp": "2025-07-25T19:30:00Z",
        "cors_enabled": True
    }

@app.get("/api/depth/status")
async def depth_status():
    """Check depth estimation API status"""
    global depth_estimator
    memory_mb = get_memory_usage()
    
    return {
        "status": "available",
        "model_loaded": depth_estimator is not None,
        "memory_usage_mb": round(memory_mb, 1),
        "memory_limit_mb": 512,
        "available_models": list(available_models.keys())
    }

@app.get("/api/depth/models")
async def get_available_models():
    """Get list of available models"""
    return {
        "models": list(available_models.keys()),
        "default": "Intel/dpt-hybrid-midas",
        "descriptions": available_models
    }

@app.post("/api/depth/estimate")
async def estimate_depth(
    file: UploadFile = File(...),
    model_name: str = "Intel/dpt-hybrid-midas"
):
    """
    Estimate depth from uploaded image (Free tier optimized)
    """
    try:
        logger.info(f"üîç Starting depth estimation (Memory: {get_memory_usage():.1f}MB)")
        
        # Memory check before processing
        current_memory = get_memory_usage()
        if current_memory > 450:  # Close to 512MB limit
            logger.warning(f"‚ö†Ô∏è High memory usage: {current_memory:.1f}MB")
            gc.collect()
            
        # Validate file
        if not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="Invalid image format")
        
        # Check file size (limit to 10MB for free tier)
        file_size = 0
        image_data = await file.read()
        file_size = len(image_data)
        
        if file_size > 10 * 1024 * 1024:  # 10MB limit
            raise HTTPException(status_code=400, detail="File too large. Please use image smaller than 10MB.")
        
        logger.info(f"üìÅ Processing image: {file_size / 1024:.1f}KB")
        
        # Load model lazily with error handling
        try:
            estimator = load_depth_estimator()
        except Exception as model_error:
            logger.error(f"‚ùå Model loading failed: {model_error}")
            raise HTTPException(
                status_code=503, 
                detail="Service temporarily unavailable. Model loading failed. Please try again in a moment."
            )
        
        # Import processing libraries
        from PIL import Image
        import torch
        import numpy as np
        import io
        import base64
        
        # Process image
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # Resize for free tier memory constraints
        max_size = 512  # Reduce from default 1024
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)
            logger.info(f"üìè Resized image to {new_size}")
        
        # Prepare inputs
        inputs = estimator['processor'](images=image, return_tensors="pt")
        
        logger.info(f"üß† Running model inference (Memory: {get_memory_usage():.1f}MB)")
        
        # Run inference
        with torch.no_grad():
            outputs = estimator['model'](**inputs)
            predicted_depth = outputs.predicted_depth
        
        # Post-process
        prediction = torch.nn.functional.interpolate(
            predicted_depth.unsqueeze(1),
            size=image.size[::-1],
            mode="bicubic",
            align_corners=False,
        )
        
        output = prediction.squeeze().cpu().numpy()
        
        # Normalize depth values based on model type
        from depth_normalization import normalize_depth_output
        normalized_depth = normalize_depth_output(output, estimator['model_name'])
        
        # Convert to 8-bit grayscale
        formatted = (normalized_depth * 255).astype("uint8")
        
        # Create depth map image
        depth_image = Image.fromarray(formatted, mode='L')
        
        # Convert to base64 for response
        img_buffer = io.BytesIO()
        depth_image.save(img_buffer, format='PNG')
        depth_b64 = base64.b64encode(img_buffer.getvalue()).decode()
        
        # Convert original to base64
        orig_buffer = io.BytesIO()
        image.save(orig_buffer, format='PNG')
        orig_b64 = base64.b64encode(orig_buffer.getvalue()).decode()
        
        # Clean up memory
        del inputs, outputs, predicted_depth, prediction
        gc.collect()
        
        logger.info(f"‚úÖ Depth estimation completed (Memory: {get_memory_usage():.1f}MB)")
        
        return JSONResponse({
            "success": True,
            "depth_map_url": f"data:image/png;base64,{depth_b64}",
            "original_url": f"data:image/png;base64,{orig_b64}",
            "model_used": estimator['model_name'],
            "resolution": f"{image.width}x{image.height}",
            "memory_usage_mb": round(get_memory_usage(), 1)
        })
        
    except Exception as e:
        logger.error(f"‚ùå Depth estimation error: {str(e)}")
        logger.error(f"‚ùå Error type: {type(e).__name__}")
        import traceback
        logger.error(f"‚ùå Full traceback: {traceback.format_exc()}")
        # Clean up on error
        gc.collect()
        
        # Return more detailed error information
        error_detail = {
            "error": str(e),
            "error_type": type(e).__name__,
            "memory_usage_mb": round(get_memory_usage(), 1),
            "suggestion": "ÁîªÂÉè„Çµ„Ç§„Ç∫„ÇíÂ∞è„Åï„Åè„Åô„Çã„Åã„ÄÅ„Åó„Å∞„Çâ„ÅèÂæÖ„Å£„Å¶„Åã„ÇâÂÜçË©¶Ë°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
        }
        raise HTTPException(status_code=500, detail=error_detail)

def main():
    port = int(os.getenv("PORT", 10000))
    logger.info(f"üöÄ Starting Free Tier Depth Estimation API on port {port}")
    logger.info(f"üíæ Initial memory usage: {get_memory_usage():.1f}MB")
    logger.info(f"üîç Environment: {os.getenv('ENVIRONMENT', 'not set')}")
    
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            log_level="info",
            access_log=True
        )
    except Exception as e:
        logger.error(f"‚ùå Server startup failed: {e}")
        raise

if __name__ == "__main__":
    main()