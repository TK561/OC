# Google Colab 完全実行コード（コピペ用）
# 1. 新しいノートブック作成
# 2. ランタイム → ランタイムのタイプを変更 → GPU → 保存
# 3. 以下を1つのセルにコピペして実行

!pip install torch torchvision transformers gradio pyngrok opencv-python-headless Pillow numpy -q

import pyngrok
from pyngrok import ngrok
import torch
import gradio as gr
from transformers import AutoImageProcessor, AutoModelForDepthEstimation
import numpy as np
from PIL import Image
import cv2

# ngrok認証
NGROK_TOKEN = "ak_30Sd307Vvyan2iewy7g5tIVl4mQ"
ngrok.set_auth_token(NGROK_TOKEN)
print("✅ ngrok認証完了")

# API初期化
class ExhibitionAPI:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"🔧 デバイス: {self.device}")
        
        print("📥 モデルダウンロード中...")
        self.processor = AutoImageProcessor.from_pretrained("depth-anything/Depth-Anything-V2-Small-hf")
        self.model = AutoModelForDepthEstimation.from_pretrained("depth-anything/Depth-Anything-V2-Small-hf")
        self.model.to(self.device)
        print("✅ モデル準備完了")
    
    def process(self, image):
        if image is None:
            return None, None
        
        try:
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            inputs = self.processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                depth = outputs.predicted_depth.squeeze().cpu().numpy()
            
            depth_normalized = ((depth - depth.min()) / (depth.max() - depth.min()) * 255).astype(np.uint8)
            depth_colored = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_VIRIDIS)
            depth_colored = cv2.cvtColor(depth_colored, cv2.COLOR_BGR2RGB)
            depth_image = Image.fromarray(depth_colored)
            
            # メモリクリーンアップ
            del inputs, outputs, depth, depth_normalized, depth_colored
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return image, depth_image
        except Exception as e:
            print(f"エラー: {e}")
            return None, None

print("🚀 API初期化中...")
api = ExhibitionAPI()

# Gradioインターフェース
interface = gr.Interface(
    fn=api.process,
    inputs=gr.Image(type="pil", label="📷 画像アップロード"),
    outputs=[gr.Image(type="pil", label="📸 元画像"), gr.Image(type="pil", label="🎨 深度マップ")],
    title="🏛️ 展示用深度推定API",
    description="🔒 完全セキュア | ✅ 外部漏洩なし | 💰 完全無料 | ⏱️ 2-5秒処理",
    allow_flagging="never"
)

# ngrok URL生成
public_url = ngrok.connect(7860)
print("\n" + "="*70)
print("🎉 展示用深度推定API起動完了!")
print(f"📡 Public URL: {public_url}")
print("\n🔧 Vercel設定手順:")
print("1. https://vercel.com/dashboard を開く")
print("2. プロジェクト → Settings → Environment Variables")
print("3. NEXT_PUBLIC_BACKEND_URL に以下を設定:")
print(f"   {public_url}")
print("4. Save → Redeployをクリック")
print("="*70)

# API起動
interface.launch(server_name="0.0.0.0", server_port=7860, share=False)